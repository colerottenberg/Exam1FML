\documentclass[10pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Page layout
\geometry{a4paper, margin=0.5in}

\begin{document}

\begin{multicols}{2}
\section*{Intro to ML}
Objective Function for Vanilla Linear Regression:
\begin{equation}
    J(\omega) = \frac{1}{2} \lVert{t - X\omega} \rVert^{2}_{2}
\end{equation}
We solve for the optimal $\omega$ by taking the derivative of the objective function with respect to $\omega$ and setting it to zero:

\begin{equation}
    \frac{\partial J(\omega)}{\partial \omega} = 0, (X^{T}X)^{-1}X^{T}t = \omega
\end{equation}

% Feature Matrix and Target Vector
\begin{equation} \label{eq:feature-target}
    X = \begin{bmatrix} 
    1 & x_{1} \\
    1 & x_{2} \\
    \vdots & \vdots \\
    1 & x_{n}
    \quad
    \end{bmatrix}
    t = \begin{bmatrix}
    t_{1} \\
    t_{2} \\
    \vdots \\
    t_{n}
    \end{bmatrix}
\end{equation}

\begin{equation}
    % Ridge Regression
    \label{eq:ridge}
    (X^{T}X + \lambda I)^{-1}X^{T}t = \omega
\end{equation}

\section*{Experimental Design and Analysis}
\subsection*{Basis Functions}

\begin{equation}
    \phi(x) = \begin{bmatrix}
    \phi_{0}(x) \\
    \phi_{1}(x) \\
    \vdots \\
    \phi_{M}(x)
    \end{bmatrix}
\end{equation}

% Radial Basis Function
\begin{equation}
    % e 
    \phi_{j}(x) = \exp -\frac{\lVert x - \mu \rVert^2}{2\sigma^{2}}
\end{equation}

% Polynomial Basis Function
\begin{equation}
    \phi_{j}(x) = x^{j}
\end{equation}
% Add more sections as needed

\subsection*{Model Selection}
% Lasso Regression
\begin{equation}
    \label{eq:lasso}
    \min_{\omega} \frac{1}{2} \lVert{t - X\omega} \rVert^{2}_{2} + \lambda \lVert{\omega} \rVert_{1}
\end{equation}

% Elastic Net
\begin{equation}
    \label{eq:elastic-net}
    \min_{\omega} \frac{1}{2} \lVert{t - X\omega} \rVert^{2}_{2} + \lambda_{1} \lVert{\omega} \rVert_{1} + \lambda_{2} \lVert{\omega} \rVert^{2}_{2}, \quad \lambda_{1} + \lambda_{2} = 1
\end{equation}

\subsection*{Metrics of Regression}
% Mean Squared Error
\begin{equation}
    \label{eq:mse}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (t_{i} - \hat{t}_{i})^{2}
\end{equation}

% Mean Absolute Error
\begin{equation}
    \label{eq:mae}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \lvert t_{i} - \hat{t}_{i} \rvert
\end{equation}

% R-Squared
\begin{equation}
    \label{eq:r-squared}
    R^{2} = 1 - \frac{\sum_{i=1}^{n} (t_{i} - \hat{t}_{i})^{2}}{\sum_{i=1}^{n} (t_{i} - \bar{t})^{2}}, \quad \bar{t} = \frac{1}{n} \sum_{i=1}^{n} t_{i}
\end{equation}

% Another R-Squared Equation
\begin{equation}
    \label{eq:r-squared-2}
    R^{2} = 1 - \frac{\lVert{t - X\omega} \rVert^{2}_{2}}{\lVert{t - \bar{t}} \rVert^{2}_{2}}
\end{equation}

% Q-Q Plot

\subsection*{Bayesian Learning}
% Bayesian Interpretation of Least Squares Regression
\begin{equation}
    \label{eq:bayesian-ls}
    \omega_{MLE} = \text{arg}_{\omega}\text{max} \prod_{i=1}^{n} p(t_{i} | x_{i}, \omega)
\end{equation}

\begin{equation}
    \label{eq:bayesian-ls-2}
    \omega_{MLE} = \text{arg}_{\omega}\text{max} \sum_{i=1}^{N} \ln p(t_{i} | x_{i}, \omega) \quad
\end{equation}

\begin{equation*}
    \label{eq:bayesian-ls-3}
    \omega_{MLE} = \text{arg}_{\omega}\text{min} \mathbb{E}_{\phi(x)} [\ln p(t | x, \omega)]
\end{equation*}

% Ridge Regression as MAP Maximum A Posteriori
\begin{equation}
    \label{eq:ridge-map}
    \omega_{\text{MAP}} \propto \text{arg}_{\omega}\text{max} \prod_{i=1}^{N} \mathcal{N}(t_i ; y_i, 1) \mathcal{N}(\omega_j ; 0 , \frac{1}{\lambda})
\end{equation}

% Conjucate Priors
\begin{enumerate}
    \item Gaussian-Gaussian
    \vspace*{-6pt}
    \item Gaussian-Exponential
    \vspace*{-6pt}
    \item Gaussian-Gamma
    \vspace*{-6pt}
    \item Gaussian-Beta
    \vspace*{-6pt}
    \item Gaussian-Dirichlet
    \vspace*{-6pt}
    \item Gaussian-Wishart
    \vspace*{-6pt}
    \item Gaussian-Inverse Wishart
    \vspace*{-6pt}
    \item Gaussian-Student's t
    \vspace*{-6pt}
    \item Gaussian-Laplace
    \vspace*{-6pt}
    \item Gaussian-Cauchy
\end{enumerate}

\section*{Generative Models}

% Probabilistic Generative Model
\begin{equation}
    \label{eq:generative}
    p(t | x, \omega) = \mathcal{N}(t ; \omega^{T}\phi(x), \beta^{-1})
\end{equation}

% Gaussian Mixture Model
\begin{equation*}
    p(x | \omega) = \sum_{k=1}^{K} \pi_{k} \mathcal{N}(x ; \mu_{k}, \Sigma_{k})
\end{equation*}

\begin{equation*}
    \Theta = \{ \pi_{1}, \pi_{2}, \ldots, \pi_{K}, \mu_{1}, \mu_{2}, \ldots, \mu_{K}, \Sigma_{1}, \Sigma_{2}, \ldots, \Sigma_{K} \}, \quad \sum_{k=1}^{K} \pi_{k} = 1
\end{equation*}

% Expectation Maximization

%Observed Data Likelihood
\begin{equation*} \label{eq:observed-likelihood}
    \mathcal{L}_0 = \prod_{i=1}^{N} \sum_{k=1}^{K} \pi_{k} \mathcal{N}(x_i ; \mu_{k}, \Sigma_{k})
\end{equation*}

% Log Likelihood
\begin{equation*} \label{eq:log-likelihood}
    \ln \mathcal{L}_0 = \sum_{i=1}^{N} \ln \sum_{k=1}^{K} \pi_{k} \mathcal{N}(x_i ; \mu_{k}, \Sigma_{k})
\end{equation*}

% Hidden Latent Variable
\begin{equation*} \label{eq:hidden-latent}
    z_{i} = \text{label of the Gaussian component for the $i^{th}$ data point $x_i$}
\end{equation*}
    

\end{multicols}

\end{document}